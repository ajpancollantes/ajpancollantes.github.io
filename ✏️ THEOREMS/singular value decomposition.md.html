<!DOCTYPE html>
<html>
<head>
<script> MathJax={tex:{inlineMath: [['$', '$']]}};</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/styles/default.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/languages/go.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<style>
	img { max-width:900px; }
	.codeblock { 
	background: #B0B0B0; padding:1px 10px 0px 10px; border-radius: 5px; overflow-x:auto; 
	}
	code {
 font-family: monospace; font-size: inherit; color: #202020; 
	}
	.inlineCoed {
 font-family: monospace; font-size: inherit; color: #202020; 
	}
</style>
</head>
<body style="background: #F0F0F0;">
<div style="width:90vw; padding:5vw; margin:0px; z-index: 5; text-align:left; background-color: #DCDCDC; border-radius: 5px; position:absolute; top:0; left:0px;">
<font size="3vw"><h1 style="margin-left:0px;" id="Singular_value_decomposition">Singular value decomposition</h1>
<p>I don't know how I have not learned this until today because it is very important. </p>
<p>Given any real square matrix $A$, we can write</p>
$$
A=U \Sigma V
$$
<p>where $U$ and $V$ are orthogonal matrices and $\Sigma$ is a diagonal matrix with non negative entries.</p>
<p>The meaning of this is easy: <b>any linear transformation of a vector space can be obtained by a rigid transformation (rotation or reflection), followed by a scale change in the main axis direction (and different scales could be applied in every axis) and finally followed by another rigid transformation</b>.</p>
<p><img src="./../imagenes/Pasted image 20220413160357.png" style="width:80vw; border-radius: 3px;" >  </p>
<p></p>
<p>It is related to the <a href="./polar decomposition.md.html">polar decomposition</a>.</p>
<p></p>
<h1 style="margin-left:0px;" id="Non_square_matrices">Non square matrices</h1>
<p>This also works for non square matrices. Consider that $A$ is the $n\times m$ matrix of a transformation $T:\mathbb{R}^n \to \mathbb{R}^m$. Then again $A=U \Sigma V$ where $U$ and $V$ are square orthogonal matrices of dimension $m$ and $n$ respectively. But now, $\Sigma$  is a <b>non square</b> diagonal matrix with non-negative entries.</p>
<p>The $m$ columns of $U$ represent a orthonormal basis of $\mathbb{R}^m$, $B_U$. And the $n$ columns of $V$ are a orthonormal basis of $\mathbb{R}^n$, $B_V$. The transformation $T$ can be understood like the one which sends the $i$th element of $B_V$ to the $i$th element of $B_U$ multiplied by the (non-negative) diagonal element $\sigma_{ii}$ of $\Sigma$. <b>This happens for every linear transformation!</b></p>
<p><img src="./../imagenes/Pasted image 20220413184919.png" style="width:43vw; border-radius: 3px;" ></p>
<p></p>
<p></p>
<p>It is related to <a href="./matrix diagonalization.md.html">matrix diagonalization</a>. Indeed they are equal when the matrix is symmetric positive-demidefinite.</p>
<p>Author: Antonio J. Pan-Collantes<br>
<p><a href="mailto:antonio.pan@uca.es">antonio.pan@uca.es</a></p><br>
<p>INDEX:<br>
	<iframe src="../treeview.html" width="90%" frameBorder="0" height="900px"></iframe>
</font></div>
</body>
</html>
